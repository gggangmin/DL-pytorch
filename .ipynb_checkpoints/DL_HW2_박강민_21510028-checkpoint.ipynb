{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7352, 128, 9]) torch.Size([7352, 1])\n",
      "torch.Size([2947, 128, 9]) torch.Size([2947, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "      data = load_file(prefix + name)\n",
    "      loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "\n",
    "    trainX = Variable(torch.Tensor(trainX))\n",
    "    trainy = Variable(torch.Tensor(trainy))\n",
    "    testX = Variable(torch.Tensor(testX))\n",
    "    testy = Variable(torch.Tensor(testy))\n",
    "    print(trainX.size(), trainy.size())\n",
    "    print(testX.size(), testy.size())\n",
    "    \n",
    "    return trainX, trainy, testX, testy\n",
    "    \n",
    "\n",
    "trainX, trainy, testX, testy = load_dataset('/home/proj01/dl_data/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length) : \n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, \n",
    "        num_layers = num_layers, batch_first = True)\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to('cuda:0') \n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to('cuda:0') \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "        hn = hn.view(-1, self.hidden_size) \n",
    "        out = self.relu(hn) \n",
    "        out = self.fc_1(out) \n",
    "        out = self.relu(out) \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channel, out_channels=3, kernel_size=32, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=10, kernel_size=16, stride=1)\n",
    "        self.fc1 = nn.Linear(10 * 82, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 10 * 82)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "input_size = 9\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "num_classes = 6\n",
    "sequence_length = 128\n",
    "\n",
    "lstm = LSTM(num_classes,input_size, hidden_size, num_layers, sequence_length).to('cuda:0')\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss().to('cuda:0')\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, loss : 1.80054\n",
      "Epoch : 10, loss : 1.78456\n",
      "Epoch : 20, loss : 1.78381\n",
      "Epoch : 30, loss : 1.78118\n",
      "Epoch : 40, loss : 1.77691\n",
      "Epoch : 50, loss : 1.76865\n",
      "Epoch : 60, loss : 1.75390\n",
      "Epoch : 70, loss : 1.72953\n",
      "Epoch : 80, loss : 1.69426\n",
      "Epoch : 90, loss : 1.64773\n",
      "Epoch : 100, loss : 1.59484\n",
      "Epoch : 110, loss : 1.53903\n",
      "Epoch : 120, loss : 1.48434\n",
      "Epoch : 130, loss : 1.43632\n",
      "Epoch : 140, loss : 1.39818\n",
      "Epoch : 150, loss : 1.36866\n",
      "Epoch : 160, loss : 1.34620\n",
      "Epoch : 170, loss : 1.32876\n",
      "Epoch : 180, loss : 1.31421\n",
      "Epoch : 190, loss : 1.30107\n",
      "Epoch : 200, loss : 1.28812\n",
      "Epoch : 210, loss : 1.27451\n",
      "Epoch : 220, loss : 1.25958\n",
      "Epoch : 230, loss : 1.24283\n",
      "Epoch : 240, loss : 1.22402\n",
      "Epoch : 250, loss : 1.20305\n",
      "Epoch : 260, loss : 1.18015\n",
      "Epoch : 270, loss : 1.15601\n",
      "Epoch : 280, loss : 1.13215\n",
      "Epoch : 290, loss : 1.11091\n",
      "Epoch : 300, loss : 1.09166\n",
      "Epoch : 310, loss : 1.07420\n",
      "Epoch : 320, loss : 1.05827\n",
      "Epoch : 330, loss : 1.04362\n",
      "Epoch : 340, loss : 1.02985\n",
      "Epoch : 350, loss : 1.01690\n",
      "Epoch : 360, loss : 1.00534\n",
      "Epoch : 370, loss : 0.99472\n",
      "Epoch : 380, loss : 0.98516\n",
      "Epoch : 390, loss : 0.97614\n",
      "Epoch : 400, loss : 0.96734\n",
      "Epoch : 410, loss : 0.95851\n",
      "Epoch : 420, loss : 0.94940\n",
      "Epoch : 430, loss : 0.93977\n",
      "Epoch : 440, loss : 0.92945\n",
      "Epoch : 450, loss : 0.91800\n",
      "Epoch : 460, loss : 0.90552\n",
      "Epoch : 470, loss : 0.89255\n",
      "Epoch : 480, loss : 0.87933\n",
      "Epoch : 490, loss : 0.86605\n",
      "Epoch : 500, loss : 0.85239\n",
      "Epoch : 510, loss : 0.83852\n",
      "Epoch : 520, loss : 0.82450\n",
      "Epoch : 530, loss : 0.81041\n",
      "Epoch : 540, loss : 0.79632\n",
      "Epoch : 550, loss : 0.78245\n",
      "Epoch : 560, loss : 0.76924\n",
      "Epoch : 570, loss : 0.75687\n",
      "Epoch : 580, loss : 0.74539\n",
      "Epoch : 590, loss : 0.73490\n",
      "Epoch : 600, loss : 0.72541\n",
      "Epoch : 610, loss : 0.71708\n",
      "Epoch : 620, loss : 0.70977\n",
      "Epoch : 630, loss : 0.70328\n",
      "Epoch : 640, loss : 0.69758\n",
      "Epoch : 650, loss : 0.69244\n",
      "Epoch : 660, loss : 0.68761\n",
      "Epoch : 670, loss : 0.68292\n",
      "Epoch : 680, loss : 0.67834\n",
      "Epoch : 690, loss : 0.67392\n",
      "Epoch : 700, loss : 0.66973\n",
      "Epoch : 710, loss : 0.66572\n",
      "Epoch : 720, loss : 0.66191\n",
      "Epoch : 730, loss : 0.65832\n",
      "Epoch : 740, loss : 0.65492\n",
      "Epoch : 750, loss : 0.65175\n",
      "Epoch : 760, loss : 0.64879\n",
      "Epoch : 770, loss : 0.64606\n",
      "Epoch : 780, loss : 0.64357\n",
      "Epoch : 790, loss : 0.64132\n",
      "Epoch : 800, loss : 0.63929\n",
      "Epoch : 810, loss : 0.63745\n",
      "Epoch : 820, loss : 0.63581\n",
      "Epoch : 830, loss : 0.63424\n",
      "Epoch : 840, loss : 0.63281\n",
      "Epoch : 850, loss : 0.63147\n",
      "Epoch : 860, loss : 0.63021\n",
      "Epoch : 870, loss : 0.62904\n",
      "Epoch : 880, loss : 0.62792\n",
      "Epoch : 890, loss : 0.62686\n",
      "Epoch : 900, loss : 0.62584\n",
      "Epoch : 910, loss : 0.62492\n",
      "Epoch : 920, loss : 0.62397\n",
      "Epoch : 930, loss : 0.62305\n",
      "Epoch : 940, loss : 0.62215\n",
      "Epoch : 950, loss : 0.62126\n",
      "Epoch : 960, loss : 0.62040\n",
      "Epoch : 970, loss : 0.61957\n",
      "Epoch : 980, loss : 0.61877\n",
      "Epoch : 990, loss : 0.61799\n"
     ]
    }
   ],
   "source": [
    "# lstm train\n",
    "lstm.train()\n",
    "lstm_losses = []\n",
    "for e in range(epochs):\n",
    "  outputs = lstm.forward(trainX.to('cuda:0'))\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  loss = loss_function(outputs,trainy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "  \n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  if e%10 == 0:\n",
    "    print(\"Epoch : %d, loss : %1.5f\" % (e, loss.item()))\n",
    "    lstm_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(0.6376)\n"
     ]
    }
   ],
   "source": [
    "# lstm eval\n",
    "def accuracy(outputs, labels):\n",
    "  _,preds = torch.max(outputs,dim=1)\n",
    "  return torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
    "\n",
    "lstm.eval()\n",
    "out = lstm(testX.to('cuda:0'))\n",
    "\n",
    "loss = loss_function(out,testy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "acc = accuracy(out,testy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "\n",
    "print(loss,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "input_size = 9\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "num_classes = 6\n",
    "sequence_length = 128\n",
    "num_channel = 9 # for cnn\n",
    "\n",
    "cnn = CNN(num_classes,num_channel).to('cuda:0')\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss().to('cuda:0')\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, loss : 1.79949\n",
      "Epoch : 10, loss : 1.70112\n",
      "Epoch : 20, loss : 1.35387\n",
      "Epoch : 30, loss : 0.96266\n",
      "Epoch : 40, loss : 0.63170\n",
      "Epoch : 50, loss : 0.43633\n",
      "Epoch : 60, loss : 0.33833\n",
      "Epoch : 70, loss : 0.27876\n",
      "Epoch : 80, loss : 0.24041\n",
      "Epoch : 90, loss : 0.20707\n",
      "Epoch : 100, loss : 0.18199\n",
      "Epoch : 110, loss : 0.16364\n",
      "Epoch : 120, loss : 0.15251\n",
      "Epoch : 130, loss : 0.13975\n",
      "Epoch : 140, loss : 0.13345\n",
      "Epoch : 150, loss : 0.12880\n",
      "Epoch : 160, loss : 0.12547\n",
      "Epoch : 170, loss : 0.12268\n",
      "Epoch : 180, loss : 0.12044\n",
      "Epoch : 190, loss : 0.11866\n",
      "Epoch : 200, loss : 0.11717\n",
      "Epoch : 210, loss : 0.11593\n",
      "Epoch : 220, loss : 0.11529\n",
      "Epoch : 230, loss : 0.11412\n",
      "Epoch : 240, loss : 0.11295\n",
      "Epoch : 250, loss : 0.11214\n",
      "Epoch : 260, loss : 0.11140\n",
      "Epoch : 270, loss : 0.11073\n",
      "Epoch : 280, loss : 0.11293\n",
      "Epoch : 290, loss : 0.11041\n",
      "Epoch : 300, loss : 0.10929\n",
      "Epoch : 310, loss : 0.10844\n",
      "Epoch : 320, loss : 0.10800\n",
      "Epoch : 330, loss : 0.10744\n",
      "Epoch : 340, loss : 0.11034\n",
      "Epoch : 350, loss : 0.10759\n",
      "Epoch : 360, loss : 0.10641\n",
      "Epoch : 370, loss : 0.10594\n",
      "Epoch : 380, loss : 0.10541\n",
      "Epoch : 390, loss : 0.10499\n",
      "Epoch : 400, loss : 0.10490\n",
      "Epoch : 410, loss : 0.10434\n",
      "Epoch : 420, loss : 0.10522\n",
      "Epoch : 430, loss : 0.10371\n",
      "Epoch : 440, loss : 0.10324\n",
      "Epoch : 450, loss : 0.10263\n",
      "Epoch : 460, loss : 0.10241\n",
      "Epoch : 470, loss : 0.10268\n",
      "Epoch : 480, loss : 0.10250\n",
      "Epoch : 490, loss : 0.10217\n",
      "Epoch : 500, loss : 0.10143\n",
      "Epoch : 510, loss : 0.10145\n",
      "Epoch : 520, loss : 0.10134\n",
      "Epoch : 530, loss : 0.10337\n",
      "Epoch : 540, loss : 0.10057\n",
      "Epoch : 550, loss : 0.09990\n",
      "Epoch : 560, loss : 0.09891\n",
      "Epoch : 570, loss : 0.09856\n",
      "Epoch : 580, loss : 0.09823\n",
      "Epoch : 590, loss : 0.09770\n",
      "Epoch : 600, loss : 0.10141\n",
      "Epoch : 610, loss : 0.09979\n",
      "Epoch : 620, loss : 0.10038\n",
      "Epoch : 630, loss : 0.09707\n",
      "Epoch : 640, loss : 0.09683\n",
      "Epoch : 650, loss : 0.09619\n",
      "Epoch : 660, loss : 0.09577\n",
      "Epoch : 670, loss : 0.09544\n",
      "Epoch : 680, loss : 0.09515\n",
      "Epoch : 690, loss : 0.09482\n",
      "Epoch : 700, loss : 0.09773\n",
      "Epoch : 710, loss : 0.09807\n",
      "Epoch : 720, loss : 0.09612\n",
      "Epoch : 730, loss : 0.09672\n",
      "Epoch : 740, loss : 0.09494\n",
      "Epoch : 750, loss : 0.09362\n",
      "Epoch : 760, loss : 0.09289\n",
      "Epoch : 770, loss : 0.09251\n",
      "Epoch : 780, loss : 0.09258\n",
      "Epoch : 790, loss : 0.09238\n",
      "Epoch : 800, loss : 0.09254\n",
      "Epoch : 810, loss : 0.09274\n",
      "Epoch : 820, loss : 0.09675\n",
      "Epoch : 830, loss : 0.09175\n",
      "Epoch : 840, loss : 0.09144\n",
      "Epoch : 850, loss : 0.09047\n",
      "Epoch : 860, loss : 0.08994\n",
      "Epoch : 870, loss : 0.08976\n",
      "Epoch : 880, loss : 0.08931\n",
      "Epoch : 890, loss : 0.08907\n",
      "Epoch : 900, loss : 0.08874\n",
      "Epoch : 910, loss : 0.08983\n",
      "Epoch : 920, loss : 0.09182\n",
      "Epoch : 930, loss : 0.09179\n",
      "Epoch : 940, loss : 0.08963\n",
      "Epoch : 950, loss : 0.08971\n",
      "Epoch : 960, loss : 0.09092\n",
      "Epoch : 970, loss : 0.08683\n",
      "Epoch : 980, loss : 0.08657\n",
      "Epoch : 990, loss : 0.08618\n"
     ]
    }
   ],
   "source": [
    "# cnn train\n",
    "cnn.train()\n",
    "cnn_losses = []\n",
    "for e in range(epochs):\n",
    "  outputs = cnn.forward(trainX.transpose(1,2).to('cuda:0'))\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  loss = loss_function(outputs,trainy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "  \n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  if e%10 == 0:\n",
    "    print(\"Epoch : %d, loss : %1.5f\" % (e, loss.item()))\n",
    "    cnn_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0422, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(0.6145)\n"
     ]
    }
   ],
   "source": [
    "# cnn eval\n",
    "def accuracy(outputs, labels):\n",
    "  _,preds = torch.max(outputs,dim=1)\n",
    "  return torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
    "\n",
    "cnn.eval()\n",
    "out = lstm(testX.to('cuda:0'))\n",
    "\n",
    "loss = loss_function(out,testy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "acc = accuracy(out,testy.squeeze(dim=1).type(torch.LongTensor).to('cuda:0'))\n",
    "\n",
    "print(loss,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1609c5d6daf415add5fd1f90da23d5406f9079b60a4e5640100862a3157a35d6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
